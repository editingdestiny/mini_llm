# ğŸ¤– Mini LLM

A lightweight, educational implementation of a Large Language Model from scratch using PyTorch. This project demonstrates the complete pipeline of building, training, and fine-tuning a transformer-based language model.

## ğŸŒŸ Features

- **Custom Tokenizer**: BPE (Byte Pair Encoding) tokenizer trained from scratch
- **Transformer Architecture**: GPT-style decoder-only model with multi-head attention
- **Two-Stage Training**:
  - **Pretraining**: Next-token prediction on raw text corpus
  - **Supervised Fine-Tuning (SFT)**: Instruction-following using question-answer pairs
- **Text Generation**: Sample text from your trained model with customizable parameters

## ğŸ“ Project Structure

```
mini-llm/
â”œâ”€â”€ mini_tokenizer.py      # Custom BPE tokenizer implementation
â”œâ”€â”€ mini_model.py          # Transformer model architecture
â”œâ”€â”€ dataset.py             # Dataset loader for pretraining
â”œâ”€â”€ sft_dataset.py         # Dataset loader for supervised fine-tuning
â”œâ”€â”€ train_tokenizer.py     # Script to train the tokenizer
â”œâ”€â”€ train_pretrain.py      # Pretraining script
â”œâ”€â”€ train_sft.py           # Fine-tuning script
â”œâ”€â”€ generate_text.py       # Text generation from trained model
â”œâ”€â”€ test_model.py          # Model testing utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ pretrain.txt   # Raw text corpus for pretraining
â”‚   â”œâ”€â”€ sft/
â”‚   â”‚   â””â”€â”€ instructions.jsonl  # Instruction-response pairs
â”‚   â””â”€â”€ tokenizer.json     # Trained tokenizer vocabulary
â””â”€â”€ checkpoints/
    â”œâ”€â”€ mini_gpt_pretrained.pt  # Pretrained model weights
    â””â”€â”€ mini_gpt_sft.pt         # Fine-tuned model weights
```

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install torch
```

### Training Pipeline

#### 1. Train the Tokenizer

First, train a BPE tokenizer on your text corpus:

```bash
python train_tokenizer.py
```

This creates a vocabulary and saves it to `data/tokenizer.json`.

#### 2. Pretrain the Model

Train the model on next-token prediction:

```bash
python train_pretrain.py
```

The model learns general language patterns and saves checkpoints to `checkpoints/mini_gpt_pretrained.pt`.

#### 3. Supervised Fine-Tuning

Fine-tune the pretrained model on instruction-following tasks:

```bash
python train_sft.py
```

This aligns the model to follow instructions using the data in `data/sft/instructions.jsonl`.

#### 4. Generate Text

Generate text from your trained model:

```bash
python generate_text.py
```

## ğŸ—ï¸ Model Architecture

- **Type**: GPT-style Decoder-only Transformer
- **Embedding Dimension**: 128
- **Attention Heads**: 4
- **Layers**: 4
- **Vocabulary Size**: 512 tokens
- **Context Length**: 128 tokens

## ğŸ“Š Training Configuration

### Pretraining
- **Objective**: Next-token prediction
- **Batch Size**: 16
- **Epochs**: 20
- **Learning Rate**: 3e-4
- **Optimizer**: AdamW

### Fine-Tuning
- **Objective**: Instruction following
- **Batch Size**: 4
- **Epochs**: 10
- **Learning Rate**: 1e-4
- **Optimizer**: AdamW

## ğŸ¯ Use Cases

This project is perfect for:
- **Learning**: Understanding transformer architecture and LLM training pipeline
- **Experimentation**: Testing new training techniques on a small scale
- **Education**: Teaching others about language model fundamentals
- **Prototyping**: Quick iterations before scaling to larger models

## ğŸ”§ Customization

### Modify Model Size

Edit `mini_model.py` to change:
- `d_model`: Embedding dimension
- `n_heads`: Number of attention heads
- `n_layers`: Number of transformer blocks
- `max_len`: Maximum sequence length

### Add Your Own Data

- **Pretraining**: Add text files to `data/raw/pretrain.txt`
- **Fine-tuning**: Add instruction pairs to `data/sft/instructions.jsonl` in the format:
  ```json
  {"instruction": "Question here", "response": "Answer here"}
  ```

## ğŸ“ˆ Monitoring Training

Both training scripts output:
- Loss per epoch
- Training progress
- Model checkpoints

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests for improvements!

## ğŸ“ License

This project is open source and available for educational purposes.

## ğŸ™ Acknowledgments

Built with PyTorch and inspired by the GPT architecture from "Attention Is All You Need" and subsequent transformer models.

---

**Note**: This is a educational implementation designed for learning purposes. For production use, consider established frameworks like Hugging Face Transformers.
